{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, target, test_size):\n",
    "    nb_samples = len(target)\n",
    "    nb_test = int(test_size * nb_samples)\n",
    "\n",
    "    shuffle = list(range(nb_samples))\n",
    "    random.shuffle(shuffle)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    \n",
    "    nb_0 = 0\n",
    "    nb_1 = 0\n",
    "\n",
    "    for idx in shuffle:\n",
    "        if nb_0 < (nb_test//2) and target[idx]==0:\n",
    "            y_test.append(0)\n",
    "            X_test.append(data[idx])\n",
    "            nb_0 += 1\n",
    "        elif nb_1 < (nb_test//2) and target[idx]==1:\n",
    "            y_test.append(1)\n",
    "            X_test.append(data[idx])\n",
    "            nb_1 += 1\n",
    "        else:\n",
    "            y_train.append(target[idx])\n",
    "            X_train.append(data[idx])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def prepareData(dataGroup, id_list, window_time):\n",
    "    \n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../data/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patientsRea.parquet\")\n",
    "\n",
    "    finalDataPath = dataPath + \"finalData/\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for encounterId in tqdm(id_list, total=len(id_list)):\n",
    "        \n",
    "        df_mask = pd.read_parquet(finalDataPath + encounterId + \"/mask.parquet\")\n",
    "        df_dynamic = pd.read_parquet(finalDataPath + encounterId + \"/dynamic.parquet\")\n",
    "        df_static = pd.read_parquet(finalDataPath + encounterId + \"/static.parquet\")\n",
    "        \n",
    "        data_patient = df_dynamic.iloc[:(window_time*24)].to_numpy()\n",
    "\n",
    "        # df_dynamic_masked = df_dynamic.iloc[:(window_time*24)].mask(df_mask.iloc[:(window_time*24)] == 0)\n",
    "\n",
    "        # # idx_variables_kept = [0,1,3,4,6,7]\n",
    "        # idx_variables_kept = list(range(0,10))\n",
    "        # df_dynamic_masked = df_dynamic_masked.iloc[:,idx_variables_kept]\n",
    "        # df_dynamic = df_dynamic.iloc[:,idx_variables_kept]\n",
    "\n",
    "\n",
    "        data.append(data_patient)\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def prepareDeathList(dataGroup, window_time):\n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../data/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patientsRea.parquet\")\n",
    "\n",
    "    df_death = pd.read_csv(dataPath + \"delais_deces.csv\")\n",
    "    \n",
    "    nb_patients = len(patients_df)\n",
    "\n",
    "    target = []\n",
    "    id_list = []\n",
    "\n",
    "    for _, row in tqdm(patients_df.iterrows(), total=nb_patients):\n",
    "        encounterId = str(row[\"encounterId\"])\n",
    "        \n",
    "        df_mask = pd.read_parquet(dataPath + \"finalData/\" + encounterId + \"/mask.parquet\")\n",
    "        total_true_values = df_mask.values.sum()\n",
    "        total_values = df_mask.values.size\n",
    "        percentageMissingValues = (total_values-total_true_values)/total_values * 100\n",
    "        \n",
    "        withdrawal_date = pd.Timestamp(row[\"withdrawal_date\"])\n",
    "        installation_date = pd.Timestamp(row[\"installation_date\"])\n",
    "        total_time_hour = (withdrawal_date - installation_date).total_seconds() / 3600 + 4\n",
    "\n",
    "        if total_time_hour >= window_time * 24 and percentageMissingValues < 40:\n",
    "            id_list.append(encounterId)\n",
    "            \n",
    "            delai_sortie_deces = df_death.loc[df_death[\"encounterId\"] == int(encounterId), \"delai_sortie_deces\"].to_numpy()[0]\n",
    "            if delai_sortie_deces <= 3:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "    \n",
    "    return target, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [00:12<00:00, 30.21it/s]\n",
      "100%|██████████| 287/287 [00:27<00:00, 10.36it/s]\n"
     ]
    }
   ],
   "source": [
    "dataGroup = \"dataECMO\"\n",
    "# dataGroup = \"dataRangueil\"\n",
    "\n",
    "window_time_days = 5\n",
    "target, id_list = prepareDeathList(dataGroup, window_time_days)\n",
    "data = prepareData(dataGroup, id_list, window_time_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 16, 118]             496\n",
      "         MaxPool1d-2               [-1, 16, 59]               0\n",
      "            Conv1d-3               [-1, 32, 57]           1,568\n",
      "         MaxPool1d-4               [-1, 32, 28]               0\n",
      "            Linear-5                  [-1, 128]         114,816\n",
      "            Linear-6                    [-1, 1]             129\n",
      "================================================================\n",
      "Total params: 117,009\n",
      "Trainable params: 117,009\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.45\n",
      "Estimated Total Size (MB): 0.49\n",
      "----------------------------------------------------------------\n",
      "Epoch 1/10, Loss: 4.60141384601593\n",
      "Epoch 2/10, Loss: 4.190762162208557\n",
      "Epoch 3/10, Loss: 4.212868243455887\n",
      "Epoch 4/10, Loss: 4.2022320330142975\n",
      "Epoch 5/10, Loss: 4.045055121183395\n",
      "Epoch 6/10, Loss: 4.457597941160202\n",
      "Epoch 7/10, Loss: 4.458828330039978\n",
      "Epoch 8/10, Loss: 4.165982156991959\n",
      "Epoch 9/10, Loss: 4.478025168180466\n",
      "Epoch 10/10, Loss: 3.962281256914139\n",
      "AUROC score: 0.4604591836734694\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(target)\n",
    "num_timesteps = 24 * window_time_days\n",
    "num_features = 10\n",
    "\n",
    "# Split data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = split_train_test(data, target, test_size=0.2)\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=10, out_channels=16, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(896, 128)  # Adjust input size based on your data dimensions\n",
    "        self.fc2 = nn.Linear(128, 1)  # For binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # For binary classification\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(num_features, num_timesteps))\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.permute(0, 2, 1))  # Permute to match input shape\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.permute(0, 2, 1))\n",
    "        true_labels.extend(labels.numpy())\n",
    "        predictions.extend(outputs.numpy())\n",
    "\n",
    "# Calculate AUROC score\n",
    "auroc = roc_auc_score(true_labels, predictions)\n",
    "print(\"AUROC score:\", auroc)\n",
    "\n",
    "#     for inputs, labels in test_loader:\n",
    "#         outputs = model(inputs.permute(0, 2, 1))\n",
    "#         predicted = torch.round(outputs)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted.squeeze() == labels).sum().item()\n",
    "\n",
    "# print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
