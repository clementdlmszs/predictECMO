{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "import torchinfo\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tnr_score(y_test, y_pred):\n",
    "    y_t = np.array(y_test)\n",
    "    y_p = np.array(y_pred)\n",
    "    tn = np.sum((1-y_t)*(1-y_p))\n",
    "    fp = np.sum(y_p*(1-y_t))\n",
    "    if (tn + fp) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(data, target, test_size, val_size):\n",
    "    nb_samples = len(target)\n",
    "    nb_test = int(test_size * nb_samples)\n",
    "    nb_val = int(val_size * nb_samples)\n",
    "\n",
    "    shuffle = list(range(nb_samples))\n",
    "    random.shuffle(shuffle)\n",
    "\n",
    "    x_train, x_test, x_val, y_train, y_test, y_val = [], [], [], [], [], []\n",
    "    \n",
    "    nb_0_test = 0\n",
    "    nb_1_test = 0\n",
    "    nb_0_val = 0\n",
    "    nb_1_val = 0\n",
    "\n",
    "    for idx in shuffle:\n",
    "        if nb_0_test < (nb_test//2) and target[idx]==0:\n",
    "            y_test.append(0)\n",
    "            x_test.append(data[idx])\n",
    "            nb_0_test += 1\n",
    "        elif nb_1_test < (nb_test//2) and target[idx]==1:\n",
    "            y_test.append(1)\n",
    "            x_test.append(data[idx])\n",
    "            nb_1_test += 1\n",
    "        elif nb_0_val < (nb_val//2) and target[idx]==0:\n",
    "            y_val.append(0)\n",
    "            x_val.append(data[idx])\n",
    "            nb_0_val += 1\n",
    "        elif nb_1_val < (nb_val//2) and target[idx]==1:\n",
    "            y_val.append(1)\n",
    "            x_val.append(data[idx])\n",
    "            nb_1_val += 1\n",
    "        else:\n",
    "            y_train.append(target[idx])\n",
    "            x_train.append(data[idx])\n",
    "    \n",
    "    return x_train, x_test, x_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "def prepareData(dataGroup, id_list, window_time):\n",
    "    \n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../dataECMO/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "\n",
    "    finalDataPath = dataPath + \"finalData/\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for encounterId in tqdm(id_list, total=len(id_list)):\n",
    "        \n",
    "        df_mask = pd.read_parquet(finalDataPath + encounterId + \"/mask.parquet\")\n",
    "        df_dynamic = pd.read_parquet(finalDataPath + encounterId + \"/dynamic.parquet\")\n",
    "        df_static = pd.read_parquet(finalDataPath + encounterId + \"/static.parquet\")\n",
    "        \n",
    "        # idx_variables_kept = [0,1,3,4,6,7]\n",
    "        if dataGroup == \"dataECMO\":\n",
    "            # idx_variables_kept = [0,1,2,3,4,5,6,7,8,9]\n",
    "            idx_variables_kept = [0,1,2,3,4,5,6,7,8]\n",
    "        else:\n",
    "            idx_variables_kept = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "        data_patient = df_dynamic.iloc[:(window_time*24), idx_variables_kept].to_numpy()\n",
    "        \n",
    "        for value in df_static.to_numpy()[0]:\n",
    "            new_column = np.ones(shape=(window_time*24,1)) * value\n",
    "            data_patient = np.append(data_patient, new_column, axis=1)\n",
    "        \n",
    "        # df_dynamic_masked = df_dynamic.iloc[:(window_time*24)].mask(df_mask.iloc[:(window_time*24)] == 0)\n",
    "\n",
    "        # # idx_variables_kept = [0,1,3,4,6,7]\n",
    "        # idx_variables_kept = list(range(0,10))\n",
    "        # df_dynamic_masked = df_dynamic_masked.iloc[:,idx_variables_kept]\n",
    "        # df_dynamic = df_dynamic.iloc[:,idx_variables_kept]\n",
    "\n",
    "\n",
    "        data.append(data_patient)\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def prepareDeathList(dataGroup, window_time, is_test_ECMO=False):\n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../dataECMO/\"\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "    \n",
    "    patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "\n",
    "    df_death = pd.read_csv(dataPath + \"delais_deces.csv\")\n",
    "    \n",
    "    nb_patients = len(patients_df)\n",
    "\n",
    "    target = []\n",
    "    id_list = []\n",
    "\n",
    "    for _, row in tqdm(patients_df.iterrows(), total=nb_patients):\n",
    "        encounterId = str(row[\"encounterId\"])\n",
    "        \n",
    "        df_mask = pd.read_parquet(dataPath + \"finalData/\" + encounterId + \"/mask.parquet\")\n",
    "        total_true_values = df_mask.values.sum()\n",
    "        total_values = df_mask.values.size\n",
    "        percentageMissingValues = (total_values-total_true_values)/total_values * 100\n",
    "        \n",
    "        withdrawal_date = pd.Timestamp(row[\"withdrawal_date\"])\n",
    "        installation_date = pd.Timestamp(row[\"installation_date\"])\n",
    "        total_time_hour = (withdrawal_date - installation_date).total_seconds() / 3600 + 4\n",
    "\n",
    "        if total_time_hour >= window_time * 24 and percentageMissingValues < 40:\n",
    "            if is_test_ECMO:\n",
    "                if installation_date.year < 2020:\n",
    "                    id_list.append(encounterId)\n",
    "                \n",
    "                    delai_sortie_deces = df_death.loc[df_death[\"encounterId\"] == int(encounterId), \"delai_sortie_deces\"].to_numpy()[0]\n",
    "                    if delai_sortie_deces <= 1:\n",
    "                        target.append(1)\n",
    "                    else:\n",
    "                        target.append(0)\n",
    "            else:\n",
    "                id_list.append(encounterId)\n",
    "                \n",
    "                delai_sortie_deces = df_death.loc[df_death[\"encounterId\"] == int(encounterId), \"delai_sortie_deces\"].to_numpy()[0]\n",
    "                if delai_sortie_deces <= 1:\n",
    "                    target.append(1)\n",
    "                else:\n",
    "                    target.append(0)\n",
    "    \n",
    "    return target, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2150/2150 [01:50<00:00, 19.38it/s]\n",
      "100%|██████████| 1794/1794 [03:33<00:00,  8.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# dataGroup = \"dataECMO\"\n",
    "dataGroup = \"dataRangueil\"\n",
    "\n",
    "window_time_days = 5\n",
    "target, id_list = prepareDeathList(dataGroup, window_time_days)\n",
    "data = prepareData(dataGroup, id_list, window_time_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D_0(nn.Module):\n",
    "    def __init__(self, num_features, num_static_features):\n",
    "        super(CNN_1D_0, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_static_features = num_static_features\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features-num_static_features, out_channels=8, kernel_size=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(480 , 4)\n",
    "        self.fc2 = nn.Linear(4+num_static_features, 1)  \n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_input = x[:, :-self.num_static_features, :]\n",
    "        static_input = x[:, -self.num_static_features:, 0]\n",
    "\n",
    "        out = self.pool(nn.functional.relu(self.conv1(cnn_input)))\n",
    "        out = self.pool(nn.functional.relu(self.conv2(out)))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = nn.functional.relu(self.fc1(out))\n",
    "        out = torch.cat((out, static_input), dim=1)\n",
    "        # out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = torch.sigmoid(self.fc2(out))  \n",
    "        return out\n",
    "    \n",
    "class CNN_1D_1(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CNN_1D_1, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=16, kernel_size=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(960 , 32)\n",
    "        self.fc2 = nn.Linear(32, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        out = self.pool(nn.functional.relu(self.conv2(out)))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = nn.functional.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        # out = torch.sigmoid(self.fc2(out))  \n",
    "        return out\n",
    "    \n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(1, 3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(1, 3), padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(3224, 4) \n",
    "        self.fc2 = nn.Linear(4, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.dropout2 = nn.Dropout(p=0.75)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        # x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        # x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        # x = nn.functional.sigmoid(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, num_static_features):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_static_features = num_static_features\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.fc1 = nn.Linear(hidden_size + num_static_features, 20)\n",
    "        # self.fc2 = nn.Linear(20, output_size)\n",
    "        self.fc2 = nn.Linear(hidden_size + num_static_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_input = x[:, :, :-self.num_static_features]\n",
    "        static_input = x[:, 0, -self.num_static_features:]\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(lstm_input, (h0, c0))\n",
    "        out = out[:, -1, :]  # Take the output of the last time step\n",
    "\n",
    "        out = torch.cat((out, static_input), dim=1)\n",
    "        # out = torch.relu(self.fc1(out))\n",
    "\n",
    "        # out = nn.functional.sigmoid(self.fc2(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "class LSTMModel2(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, num_features_dynamic, num_features_static):\n",
    "        \n",
    "        super(LSTMModel2, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_features_dynamic = num_features_dynamic\n",
    "        self.num_features_static = num_features_static\n",
    "        \n",
    "        # Create a list of LSTM layers, one for each feature\n",
    "        self.lstms = nn.ModuleList([nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) for _ in range(num_features_dynamic)])\n",
    "        \n",
    "        # Linear layer for binary classification\n",
    "        self.fc = nn.Linear(hidden_size*num_features_dynamic + num_features_static, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        lstm_outputs = []\n",
    "\n",
    "        for i in range(self.num_features_dynamic):\n",
    "            feature_input = x[:, :, i].unsqueeze(2)  # Shape: (batch_size, seq_length, 1)\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            lstm_out, _ = self.lstms[i](feature_input, (h0, c0))\n",
    "            lstm_out = lstm_out[:, -1, :]  # Get the last time step output: Shape: (batch_size, hidden_size)\n",
    "            lstm_outputs.append(lstm_out)\n",
    "        \n",
    "        # Concatenate the outputs from each LSTM\n",
    "        out = torch.cat(lstm_outputs, dim=1)  # Shape: (batch_size, hidden_size * num_features)\n",
    "        \n",
    "        static_input = x[:, 0, -self.num_features_static:]\n",
    "        out = torch.cat((out, static_input), dim=1)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        # out = nn.functional.sigmoid(self.fc(out))  # Shape: (batch_size, output_size)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, target, num_epochs, model_name, test_size, val_size, verbose, save_path, save_model, plot_train_curves):\n",
    "    \n",
    "    # x_train, x_test, x_val, y_train, y_test, y_val = split_train_test_val(data, target, test_size=test_size, val_size=val_size)\n",
    "    if test_size > 0:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=test_size)\n",
    "        while np.sum(y_test) < 2:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=test_size)\n",
    "\n",
    "        if val_size > 0:\n",
    "            x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size)\n",
    "            while np.sum(y_val) == 0:\n",
    "                x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size)\n",
    "        else:\n",
    "            x_val = np.array([])\n",
    "            y_val = np.array([])\n",
    "    else:\n",
    "        x_test = np.array([])\n",
    "        y_test = np.array([])\n",
    "        x_train, x_val, y_train, y_val = train_test_split(data, target, test_size=val_size)\n",
    "        while np.sum(y_val) < 2:\n",
    "            x_train, x_val, y_train, y_val = train_test_split(data, target, test_size=val_size)\n",
    "\n",
    "    num_samples = len(target)\n",
    "    num_timesteps = 24 * window_time_days\n",
    "    num_features = np.size(x_train,2)\n",
    "    num_features_static = 3\n",
    "    num_features_dynamic = num_features - num_features_static\n",
    "\n",
    "    \n",
    "    batch_size = 32\n",
    "\n",
    "    proportion_1 = np.sum(y_train)/np.size(y_train)\n",
    "    proportion_0 = 1 - proportion_1\n",
    "\n",
    "    class_weights = torch.tensor([1/proportion_0, 1/proportion_1], dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate the model\n",
    "    if model_name == \"CNN_1D_0\":\n",
    "        model = CNN_1D_0(num_features=num_features, num_static_features=num_features_static)\n",
    "\n",
    "        # if verbose:\n",
    "        #     print(torchinfo.summary(model, input_size=(batch_size, num_features, num_timesteps)))\n",
    "    \n",
    "    if model_name == \"CNN_1D_1\":\n",
    "        model = CNN_1D_1(num_features=num_features)\n",
    "\n",
    "    elif model_name == \"CNN2\":\n",
    "            model = CNN2()\n",
    "\n",
    "            if verbose:\n",
    "                print(torchinfo.summary(model, input_size=(batch_size, num_timesteps, num_features)))\n",
    "\n",
    "    elif model_name == \"LSTM\":\n",
    "        input_size = num_features-num_features_static\n",
    "        hidden_size = 32\n",
    "        num_layers = 2\n",
    "        output_size = 1\n",
    "\n",
    "        model = LSTMModel(input_size, hidden_size, num_layers, output_size, num_features_static)\n",
    "\n",
    "        if verbose:\n",
    "            print(torchinfo.summary(model, input_size=(batch_size, num_timesteps, num_features)))\n",
    "\n",
    "    if model_name == \"LSTM2\":\n",
    "        input_size = 1\n",
    "        hidden_size = 16\n",
    "        num_layers = 1\n",
    "        output_size = 1\n",
    "\n",
    "        model = LSTMModel2(input_size, hidden_size, num_layers, output_size, num_features_dynamic, num_features_static)\n",
    "\n",
    "        if verbose:\n",
    "            print(torchinfo.summary(model, input_size=(batch_size, num_timesteps, num_features)))\n",
    "\n",
    "\n",
    "    # criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    best_val_auroc = 0\n",
    "    val_auroc_list = []\n",
    "    train_auroc_list = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        # Training\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            \n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            prediction = nn.functional.sigmoid(outputs).detach()\n",
    "            predictions.extend(prediction.numpy())\n",
    "            \n",
    "            true_labels.extend(labels.numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_auroc = roc_auc_score(true_labels, predictions)\n",
    "        train_auroc_list.append(train_auroc)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train loss: {running_loss:.4f}, Train AUROC: {train_auroc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        if np.size(y_val) > 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "\n",
    "                    inputs = inputs.permute(0, 2, 1)\n",
    "\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "                    prediction = nn.functional.sigmoid(outputs)\n",
    "                    predictions.extend(prediction.numpy())\n",
    "\n",
    "                    true_labels.extend(labels.numpy())\n",
    "        \n",
    "            val_auroc = roc_auc_score(true_labels, predictions)\n",
    "            val_auroc_list.append(val_auroc) \n",
    "            if verbose:\n",
    "                print(f\"Validation Loss: {val_loss:.4f}, Validation AUROC: {val_auroc:.4f}\")\n",
    "\n",
    "            if val_auroc > best_val_auroc:\n",
    "                best_val_auroc = val_auroc\n",
    "                train_auroc_at_best_val_auroc = train_auroc\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "\n",
    "    if np.size(y_val) > 0:\n",
    "        state_dict = torch.load(save_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        if plot_train_curves:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            plt.plot(range(num_epochs), train_auroc_list, label='Train AUROC', color='blue')\n",
    "            plt.plot(range(num_epochs), val_auroc_list, label='Validation AUROC', color='red')\n",
    "\n",
    "            plt.xlabel('epochs')\n",
    "            plt.ylabel('auroc')\n",
    "            plt.title('Train and Val AUROC = f(epoch)')\n",
    "\n",
    "            plt.legend()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    predictions_binary = []\n",
    "\n",
    "    treshold = 0.5\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            true_labels.extend(labels.numpy())\n",
    "\n",
    "            outputs = nn.functional.sigmoid(model(inputs))\n",
    "            predictions.extend(outputs.numpy())\n",
    "            predictions_binary.extend((outputs.numpy() > treshold).astype(int))\n",
    "            \n",
    "            # print(np.round(np.array([p[0] for p in predictions]), 1))\n",
    "\n",
    "    auroc = roc_auc_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions_binary, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions_binary, zero_division=0)\n",
    "    tnr = tnr_score(true_labels, predictions_binary)\n",
    "    f1 = f1_score(true_labels, predictions_binary, zero_division=0)\n",
    "    accuracy = accuracy_score(true_labels, predictions_binary)\n",
    "    # if verbose:\n",
    "    # print(\"Test AUROC score:\", auroc)\n",
    "    if train_auroc_at_best_val_auroc > best_val_auroc:\n",
    "        return auroc, precision, recall, tnr, f1, accuracy, best_val_auroc\n",
    "    else:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train loss: 53.8362, Train AUROC: 0.5384\n",
      "Validation Loss: 6.4634, Validation AUROC: 0.6698\n",
      "Epoch 2/25, Train loss: 51.2595, Train AUROC: 0.6753\n",
      "Validation Loss: 5.9342, Validation AUROC: 0.7140\n",
      "Epoch 3/25, Train loss: 49.1360, Train AUROC: 0.7054\n",
      "Validation Loss: 5.9241, Validation AUROC: 0.7359\n",
      "Epoch 4/25, Train loss: 47.3517, Train AUROC: 0.7348\n",
      "Validation Loss: 5.5210, Validation AUROC: 0.7271\n",
      "Epoch 5/25, Train loss: 46.1555, Train AUROC: 0.7561\n",
      "Validation Loss: 5.4788, Validation AUROC: 0.7428\n",
      "Epoch 6/25, Train loss: 44.9303, Train AUROC: 0.7721\n",
      "Validation Loss: 5.3967, Validation AUROC: 0.7404\n",
      "Epoch 7/25, Train loss: 44.3378, Train AUROC: 0.7795\n",
      "Validation Loss: 5.4878, Validation AUROC: 0.7376\n",
      "Epoch 8/25, Train loss: 43.7598, Train AUROC: 0.7854\n",
      "Validation Loss: 5.5042, Validation AUROC: 0.7305\n",
      "Epoch 9/25, Train loss: 41.9905, Train AUROC: 0.8100\n",
      "Validation Loss: 5.4802, Validation AUROC: 0.7349\n",
      "Epoch 10/25, Train loss: 41.0706, Train AUROC: 0.8180\n",
      "Validation Loss: 5.6420, Validation AUROC: 0.7317\n",
      "Epoch 11/25, Train loss: 41.3602, Train AUROC: 0.8129\n",
      "Validation Loss: 5.6266, Validation AUROC: 0.7253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m best_val_auroc_all_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_train), total\u001b[38;5;241m=\u001b[39mnum_train):\n\u001b[0;32m     14\u001b[0m     \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# auroc = train_model(num_epochs=15, model_name=\"LSTM\", test_size=0.2, val_size=0.0, verbose=False)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     auroc, precision, recall, tnr, f1, accuracy, best_val_auroc  \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCNN_1D_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mplot_train_curves\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(np\u001b[38;5;241m.\u001b[39misnan(auroc)) \u001b[38;5;129;01mand\u001b[39;00m best_val_auroc \u001b[38;5;241m>\u001b[39m best_val_auroc_all_models:\n\u001b[0;32m     26\u001b[0m         aurocs \u001b[38;5;241m=\u001b[39m [auroc]\n",
      "Cell \u001b[1;32mIn[49], line 125\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(num_epochs, model_name, test_size, val_size, verbose, save_path, save_model, plot_train_curves)\u001b[0m\n\u001b[0;32m    122\u001b[0m     true_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    124\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 125\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    128\u001b[0m train_auroc \u001b[38;5;241m=\u001b[39m roc_auc_score(true_labels, predictions)\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    390\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_train = 100\n",
    "\n",
    "save_path = \"LSTMs/lstm0.pth\"\n",
    "\n",
    "aurocs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "tnrs = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "best_val_auroc_all_models = 0\n",
    "for i in tqdm(range(num_train), total=num_train):\n",
    "    \n",
    "    # auroc = train_model(num_epochs=15, model_name=\"LSTM\", test_size=0.2, val_size=0.0, verbose=False)\n",
    "    auroc, precision, recall, tnr, f1, accuracy, best_val_auroc  =  train_model(data=data,\n",
    "                                                                                target=target,\n",
    "                                                                                num_epochs=25, \n",
    "                                                                                model_name=\"CNN_1D_1\", \n",
    "                                                                                test_size=0.10, \n",
    "                                                                                val_size=0.10, \n",
    "                                                                                verbose=True, \n",
    "                                                                                save_path=save_path, \n",
    "                                                                                save_model=True, \n",
    "                                                                                plot_train_curves = True)\n",
    "    \n",
    "    if not(np.isnan(auroc)) and best_val_auroc > best_val_auroc_all_models:\n",
    "        aurocs = [auroc]\n",
    "        precisions = precision\n",
    "        recalls = [recall]\n",
    "        tnrs = [tnr]\n",
    "        accuracies = [accuracy]\n",
    "        f1s = [f1]\n",
    "        # aurocs.append(auroc)\n",
    "        # precisions.append(precision)\n",
    "        # recalls.append(recall)\n",
    "        # tnrs.append(tnr)\n",
    "        # f1s.append(f1)\n",
    "        # accuracies.append(accuracy)\n",
    "        best_val_auroc_all_models = best_val_auroc\n",
    "        print(f\"New best val_auroc: {best_val_auroc_all_models}\")\n",
    "    \n",
    "    print(f\"Test AUROC with best model: {np.mean(aurocs):.4f}\")\n",
    "\n",
    "print(f\"AUROC: {np.mean(aurocs):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f}\")\n",
    "print(f\"Specificity: {np.mean(tnrs):.4f}\")\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1s):.4f}\")\n",
    "print(f\"num_algos: {np.size(aurocs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [00:05<00:00, 31.53it/s]\n",
      "100%|██████████| 69/69 [00:05<00:00, 13.08it/s]\n"
     ]
    }
   ],
   "source": [
    "window_time_days = 5\n",
    "target_ECMO_test, id_list_ECMO_test = prepareDeathList(\"dataECMO\", window_time_days, is_test_ECMO=True)\n",
    "data_ECMO_test = prepareData(\"dataECMO\", id_list_ECMO_test, window_time_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
