{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "import torchinfo\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tnr_score(y_test, y_pred):\n",
    "    y_t = np.array(y_test)\n",
    "    y_p = np.array(y_pred)\n",
    "    tn = np.sum((1-y_t)*(1-y_p))\n",
    "    fp = np.sum(y_p*(1-y_t))\n",
    "    if (tn + fp) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(data, target, test_size, val_size):\n",
    "    nb_samples = len(target)\n",
    "    nb_test = int(test_size * nb_samples)\n",
    "    nb_val = int(val_size * nb_samples)\n",
    "\n",
    "    shuffle = list(range(nb_samples))\n",
    "    random.shuffle(shuffle)\n",
    "\n",
    "    x_train, x_test, x_val, y_train, y_test, y_val = [], [], [], [], [], []\n",
    "    \n",
    "    nb_0_test = 0\n",
    "    nb_1_test = 0\n",
    "    nb_0_val = 0\n",
    "    nb_1_val = 0\n",
    "\n",
    "    for idx in shuffle:\n",
    "        if nb_0_test < (nb_test//2) and target[idx]==0:\n",
    "            y_test.append(0)\n",
    "            x_test.append(data[idx])\n",
    "            nb_0_test += 1\n",
    "        elif nb_1_test < (nb_test//2) and target[idx]==1:\n",
    "            y_test.append(1)\n",
    "            x_test.append(data[idx])\n",
    "            nb_1_test += 1\n",
    "        elif nb_0_val < (nb_val//2) and target[idx]==0:\n",
    "            y_val.append(0)\n",
    "            x_val.append(data[idx])\n",
    "            nb_0_val += 1\n",
    "        elif nb_1_val < (nb_val//2) and target[idx]==1:\n",
    "            y_val.append(1)\n",
    "            x_val.append(data[idx])\n",
    "            nb_1_val += 1\n",
    "        else:\n",
    "            y_train.append(target[idx])\n",
    "            x_train.append(data[idx])\n",
    "    \n",
    "    return x_train, x_test, x_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "def prepareData(dataGroup, id_list, window_time):\n",
    "    \n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../dataECMO/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patientsRea.parquet\")\n",
    "\n",
    "    finalDataPath = dataPath + \"finalData/\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for encounterId in tqdm(id_list, total=len(id_list)):\n",
    "        \n",
    "        df_mask = pd.read_parquet(finalDataPath + encounterId + \"/mask.parquet\")\n",
    "        df_dynamic = pd.read_parquet(finalDataPath + encounterId + \"/dynamic.parquet\")\n",
    "        df_static = pd.read_parquet(finalDataPath + encounterId + \"/static.parquet\")\n",
    "        \n",
    "        # idx_variables_kept = [0,1,3,4,6,7]\n",
    "        idx_variables_kept = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "        data_patient = df_dynamic.iloc[:(window_time*24), idx_variables_kept].to_numpy()\n",
    "        \n",
    "        for value in df_static.to_numpy()[0]:\n",
    "            new_column = np.ones(shape=(window_time*24,1)) * value\n",
    "            data_patient = np.append(data_patient, new_column, axis=1)\n",
    "        \n",
    "        # df_dynamic_masked = df_dynamic.iloc[:(window_time*24)].mask(df_mask.iloc[:(window_time*24)] == 0)\n",
    "\n",
    "        # # idx_variables_kept = [0,1,3,4,6,7]\n",
    "        # idx_variables_kept = list(range(0,10))\n",
    "        # df_dynamic_masked = df_dynamic_masked.iloc[:,idx_variables_kept]\n",
    "        # df_dynamic = df_dynamic.iloc[:,idx_variables_kept]\n",
    "\n",
    "\n",
    "        data.append(data_patient)\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def prepareDeathList(dataGroup, window_time):\n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../dataECMO/\"\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "    \n",
    "    patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "\n",
    "    df_death = pd.read_csv(dataPath + \"delais_deces.csv\")\n",
    "    \n",
    "    nb_patients = len(patients_df)\n",
    "\n",
    "    target = []\n",
    "    id_list = []\n",
    "\n",
    "    for _, row in tqdm(patients_df.iterrows(), total=nb_patients):\n",
    "        encounterId = str(row[\"encounterId\"])\n",
    "        \n",
    "        df_mask = pd.read_parquet(dataPath + \"finalData/\" + encounterId + \"/mask.parquet\")\n",
    "        total_true_values = df_mask.values.sum()\n",
    "        total_values = df_mask.values.size\n",
    "        percentageMissingValues = (total_values-total_true_values)/total_values * 100\n",
    "        \n",
    "        withdrawal_date = pd.Timestamp(row[\"withdrawal_date\"])\n",
    "        installation_date = pd.Timestamp(row[\"installation_date\"])\n",
    "        total_time_hour = (withdrawal_date - installation_date).total_seconds() / 3600 + 4\n",
    "\n",
    "        if total_time_hour >= window_time * 24 and percentageMissingValues < 40:\n",
    "            id_list.append(encounterId)\n",
    "            \n",
    "            delai_sortie_deces = df_death.loc[df_death[\"encounterId\"] == int(encounterId), \"delai_sortie_deces\"].to_numpy()[0]\n",
    "            if delai_sortie_deces <= 1:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "    \n",
    "    return target, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [00:03<00:00, 51.18it/s]\n",
      "100%|██████████| 153/153 [00:10<00:00, 14.26it/s]\n"
     ]
    }
   ],
   "source": [
    "dataGroup = \"dataECMO\"\n",
    "# dataGroup = \"dataRangueil\"\n",
    "\n",
    "window_time_days = 5\n",
    "target, id_list = prepareDeathList(dataGroup, window_time_days)\n",
    "data = prepareData(dataGroup, id_list, window_time_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs, model_name, test_size, val_size, verbose, save_path, save_model):\n",
    "    \n",
    "    # x_train, x_test, x_val, y_train, y_test, y_val = split_train_test_val(data, target, test_size=test_size, val_size=val_size)\n",
    "   \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=test_size)\n",
    "    while np.sum(y_test) < 2:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=test_size)\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size)\n",
    "    while np.sum(y_val) == 0:\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size)\n",
    "\n",
    "    num_samples = len(target)\n",
    "    num_timesteps = 24 * window_time_days\n",
    "    num_features = np.size(x_train,2)\n",
    "    num_static_features = 3\n",
    "    \n",
    "    batch_size = 32\n",
    "\n",
    "    proportion_1 = np.sum(y_train)/np.size(y_train)\n",
    "    proportion_0 = 1 - proportion_1\n",
    "\n",
    "    class_weights = torch.tensor([1/proportion_0, 1/proportion_1], dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the CNN model\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 = nn.Conv1d(in_channels=num_features-num_static_features, out_channels=8, kernel_size=1)\n",
    "            self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "            self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=1)\n",
    "            self.fc1 = nn.Linear(480 , 16)  # Adjust input size based on your data dimensions\n",
    "            self.fc2 = nn.Linear(16+num_static_features, 1)  # For binary classification\n",
    "\n",
    "        def forward(self, x):\n",
    "            lstm_input = x[:, :-num_static_features, :]\n",
    "            static_input = x[:, -num_static_features:, 0]\n",
    "\n",
    "            out = self.pool(nn.functional.relu(self.conv1(lstm_input)))\n",
    "            out = self.pool(nn.functional.relu(self.conv2(out)))\n",
    "            out = torch.flatten(out, 1)\n",
    "            out = nn.functional.relu(self.fc1(out))\n",
    "            out = torch.cat((out, static_input), dim=1)\n",
    "            out = self.fc2(out)\n",
    "            # out = torch.sigmoid(self.fc2(out))  \n",
    "            return out\n",
    "\n",
    "    class CNN2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN2, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(1, 3), padding=1)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "            self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(1, 3), padding=1)\n",
    "            self.fc1 = nn.Linear(5760, 4)  # Adjust input size based on your data dimensions\n",
    "            self.fc2 = nn.Linear(4, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(1)  # Add a channel dimension\n",
    "            x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "            x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "            # x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = nn.functional.relu(self.fc1(x))\n",
    "            # x = nn.functional.sigmoid(self.fc2(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size, num_static_features):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            # self.fc1 = nn.Linear(hidden_size + num_static_features, 20)\n",
    "            # self.fc2 = nn.Linear(20, output_size)\n",
    "            self.fc2 = nn.Linear(hidden_size + num_static_features, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            lstm_input = x[:, :, :-num_static_features]\n",
    "            static_input = x[:, 0, -num_static_features:]\n",
    "\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            \n",
    "            out, _ = self.lstm(lstm_input, (h0, c0))\n",
    "            out = out[:, -1, :]  # Take the output of the last time step\n",
    "\n",
    "            out = torch.cat((out, static_input), dim=1)\n",
    "            # out = torch.relu(self.fc1(out))\n",
    "\n",
    "            # out = nn.functional.sigmoid(self.fc2(out))\n",
    "            out = self.fc2(out)\n",
    "            return out\n",
    "    \n",
    "    # Instantiate the model\n",
    "    if model_name == \"CNN\":\n",
    "        model = CNN()\n",
    "\n",
    "        # if verbose:\n",
    "        #     print(torchinfo.summary(model, input_size=(batch_size, num_features, num_timesteps)))\n",
    "    \n",
    "    if model_name == \"CNN2\":\n",
    "            model = CNN2()\n",
    "\n",
    "            if verbose:\n",
    "                print(torchinfo.summary(model, input_size=(batch_size, num_timesteps, num_features)))\n",
    "\n",
    "    elif model_name == \"LSTM\":\n",
    "        input_size = num_features-num_static_features\n",
    "        hidden_size = 32\n",
    "        num_layers = 2\n",
    "        output_size = 1\n",
    "\n",
    "        model = LSTMModel(input_size, hidden_size, num_layers, output_size, num_static_features)\n",
    "\n",
    "        if verbose:\n",
    "            print(torchinfo.summary(model, input_size=(batch_size, num_timesteps, num_features)))\n",
    "\n",
    "\n",
    "    class FocalLoss(nn.Module):\n",
    "        def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "            super(FocalLoss, self).__init__()\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "            self.reduction = reduction\n",
    "\n",
    "        def forward(self, inputs, targets):\n",
    "            # Apply sigmoid to inputs if not using BCEWithLogitsLoss\n",
    "            inputs = torch.sigmoid(inputs)\n",
    "            \n",
    "            # Flatten the inputs and targets\n",
    "            inputs = inputs.view(-1)\n",
    "            targets = targets.view(-1)\n",
    "            \n",
    "            # Compute the binary cross entropy loss\n",
    "            BCE_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "            \n",
    "            # Compute the focal loss component\n",
    "            pt = torch.where(targets == 1, inputs, 1 - inputs)\n",
    "            F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "            \n",
    "            if self.reduction == 'mean':\n",
    "                return F_loss.mean()\n",
    "            elif self.reduction == 'sum':\n",
    "                return F_loss.sum()\n",
    "            else:\n",
    "                return F_loss\n",
    "    # Define loss function and optimizer\n",
    "    # criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "    # Define weighted binary cross-entropy loss function\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "    # criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # inputs = inputs.permute(0, 2, 1)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            predicted = torch.round(nn.functional.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            train_accuracy = 100 * correct / total\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train loss: {running_loss}, Train Accuracy: {train_accuracy}%\")\n",
    "\n",
    "        # Validation\n",
    "        if np.size(y_val) > 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    # inputs = inputs.permute(0, 2, 1)\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    val_loss += criterion(outputs, labels).item()\n",
    "                    predicted = torch.round(nn.functional.sigmoid(outputs))\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    val_accuracy = 100 * correct / total\n",
    "            if verbose:\n",
    "                print(f\"Validation Loss: {val_loss}, Accuracy on validation set: {val_accuracy}%\")\n",
    "\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "\n",
    "\n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    predictions_binary = []\n",
    "\n",
    "    treshold = 0.5\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # inputs = inputs.permute(0, 2, 1)\n",
    "            true_labels.extend(labels.numpy())\n",
    "\n",
    "            outputs = nn.functional.sigmoid(model(inputs))\n",
    "            predictions.extend(outputs.numpy())\n",
    "            predictions_binary.extend((outputs.numpy() > treshold).astype(int))\n",
    "            \n",
    "            print(np.round(np.array([p[0] for p in predictions]), 1))\n",
    "\n",
    "    # Calculate AUROC score\n",
    "    auroc = roc_auc_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions_binary, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions_binary, zero_division=0)\n",
    "    tnr = tnr_score(true_labels, predictions_binary)\n",
    "    f1 = f1_score(true_labels, predictions_binary, zero_division=0)\n",
    "    accuracy = accuracy_score(true_labels, predictions_binary)\n",
    "    # if verbose:\n",
    "    # print(\"Test AUROC score:\", auroc)\n",
    "    return auroc, precision, recall, tnr, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<00:57,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.5 0.5 0.7 0.6 0.5 0.9 0.5 0.4 0.8 0.8 0.8 0.5 0.6 0.5 0.5 0.4\n",
      " 0.5 0.8 0.7 0.4 0.9 0.6 0.3 0.4 0.5 0.7 0.5 0.4 0.4]\n",
      "AUROC: 0.6974789915966387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:11<00:46,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6 0.6 0.5 0.7 0.5 0.5 0.6 0.5 0.5 0.6 0.5 0.8 0.5 0.5 0.6 0.6 0.5 0.5\n",
      " 0.5 0.9 0.5 0.8 0.5 0.8 0.6 0.5 0.6 0.7 0.7 0.5 0.7]\n",
      "AUROC: 0.6667219519386702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:17<00:39,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6 0.8 0.7 0.6 0.6 0.6 0.6 0.7 0.6 0.7 0.9 0.5 0.7 0.5 0.6 0.8 0.7 0.8\n",
      " 0.9 0.6 0.5 0.6 0.6 0.6 0.5 0.7 0.5 0.7 0.8 0.6 0.8]\n",
      "AUROC: 0.6265800667245456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:22<00:32,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.4 0.3 0.7 0.3 0.3 0.5 0.8 0.3 0.8 0.5 0.4 0.5 0.4 0.6 0.9 0.3 0.7\n",
      " 0.4 0.4 0.7 0.4 0.4 0.3 0.9 0.8 0.3 0.5 0.6 0.5 0.5]\n",
      "AUROC: 0.5817771553065672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:25<00:38,  6.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m f1s \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_train), total\u001b[38;5;241m=\u001b[39mnum_train):\n\u001b[0;32m     13\u001b[0m     \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# auroc = train_model(num_epochs=15, model_name=\"LSTM\", test_size=0.2, val_size=0.0, verbose=False)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     auroc, precision, recall, tnr, f1, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     aurocs\u001b[38;5;241m.\u001b[39mappend(auroc)\n\u001b[0;32m     18\u001b[0m     precisions\u001b[38;5;241m.\u001b[39mappend(precision)\n",
      "Cell \u001b[1;32mIn[62], line 192\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(num_epochs, model_name, test_size, val_size, verbose, save_path, save_model)\u001b[0m\n\u001b[0;32m    189\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    190\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m--> 192\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    194\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_train = 10\n",
    "\n",
    "save_path = \"LSTMs/lstm0.pth\"\n",
    "\n",
    "aurocs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "tnrs = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for i in tqdm(range(num_train), total=num_train):\n",
    "    \n",
    "    # auroc = train_model(num_epochs=15, model_name=\"LSTM\", test_size=0.2, val_size=0.0, verbose=False)\n",
    "    auroc, precision, recall, tnr, f1, accuracy = train_model(num_epochs=50, model_name=\"LSTM\", test_size=0.1, val_size=0.1, verbose=False, save_path=save_path, save_model=True)\n",
    "    \n",
    "    aurocs.append(auroc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    tnrs.append(tnr)\n",
    "    f1s.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"AUROC: {np.mean(aurocs)}\")\n",
    "\n",
    "print(f\"AUROC: {np.mean(aurocs):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f}\")\n",
    "print(f\"Specificity: {np.mean(tnrs):.4f}\")\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[244], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241m.\u001b[39msize([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
