{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "import torchinfo\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(data, target, test_size, val_size):\n",
    "    nb_samples = len(target)\n",
    "    nb_test = int(test_size * nb_samples)\n",
    "    nb_val = int(val_size * nb_samples)\n",
    "\n",
    "    shuffle = list(range(nb_samples))\n",
    "    random.shuffle(shuffle)\n",
    "\n",
    "    x_train, x_test, x_val, y_train, y_test, y_val = [], [], [], [], [], []\n",
    "    \n",
    "    nb_0_test = 0\n",
    "    nb_1_test = 0\n",
    "    nb_0_val = 0\n",
    "    nb_1_val = 0\n",
    "\n",
    "    for idx in shuffle:\n",
    "        if nb_0_test < (nb_test//2) and target[idx]==0:\n",
    "            y_test.append(0)\n",
    "            x_test.append(data[idx])\n",
    "            nb_0_test += 1\n",
    "        elif nb_1_test < (nb_test//2) and target[idx]==1:\n",
    "            y_test.append(1)\n",
    "            x_test.append(data[idx])\n",
    "            nb_1_test += 1\n",
    "        elif nb_0_val < (nb_val//2) and target[idx]==0:\n",
    "            y_val.append(0)\n",
    "            x_val.append(data[idx])\n",
    "            nb_0_val += 1\n",
    "        elif nb_1_val < (nb_val//2) and target[idx]==1:\n",
    "            y_val.append(1)\n",
    "            x_val.append(data[idx])\n",
    "            nb_1_val += 1\n",
    "        else:\n",
    "            y_train.append(target[idx])\n",
    "            x_train.append(data[idx])\n",
    "    \n",
    "    return x_train, x_test, x_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "def prepareData(dataGroup, id_list, window_time):\n",
    "    \n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../data/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patientsRea.parquet\")\n",
    "\n",
    "    finalDataPath = dataPath + \"finalData/\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for encounterId in tqdm(id_list, total=len(id_list)):\n",
    "        \n",
    "        df_mask = pd.read_parquet(finalDataPath + encounterId + \"/mask.parquet\")\n",
    "        df_dynamic = pd.read_parquet(finalDataPath + encounterId + \"/dynamic.parquet\")\n",
    "        df_static = pd.read_parquet(finalDataPath + encounterId + \"/static.parquet\")\n",
    "        \n",
    "        idx_variables_kept = [0,1,3,4,6,7]\n",
    "\n",
    "        data_patient = df_dynamic.iloc[:(window_time*24), idx_variables_kept].to_numpy()\n",
    "        \n",
    "        for value in df_static.to_numpy()[0]:\n",
    "            new_column = np.ones(shape=(window_time*24,1)) * value\n",
    "            data_patient = np.append(data_patient, new_column, axis=1)\n",
    "        \n",
    "        # df_dynamic_masked = df_dynamic.iloc[:(window_time*24)].mask(df_mask.iloc[:(window_time*24)] == 0)\n",
    "\n",
    "        # # idx_variables_kept = [0,1,3,4,6,7]\n",
    "        # idx_variables_kept = list(range(0,10))\n",
    "        # df_dynamic_masked = df_dynamic_masked.iloc[:,idx_variables_kept]\n",
    "        # df_dynamic = df_dynamic.iloc[:,idx_variables_kept]\n",
    "\n",
    "\n",
    "        data.append(data_patient)\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def prepareDeathList(dataGroup, window_time):\n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../data/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "        patients_df = pd.read_parquet(dataPath + \"patientsRea.parquet\")\n",
    "\n",
    "    df_death = pd.read_csv(dataPath + \"delais_deces.csv\")\n",
    "    \n",
    "    nb_patients = len(patients_df)\n",
    "\n",
    "    target = []\n",
    "    id_list = []\n",
    "\n",
    "    for _, row in tqdm(patients_df.iterrows(), total=nb_patients):\n",
    "        encounterId = str(row[\"encounterId\"])\n",
    "        \n",
    "        df_mask = pd.read_parquet(dataPath + \"finalData/\" + encounterId + \"/mask.parquet\")\n",
    "        total_true_values = df_mask.values.sum()\n",
    "        total_values = df_mask.values.size\n",
    "        percentageMissingValues = (total_values-total_true_values)/total_values * 100\n",
    "        \n",
    "        withdrawal_date = pd.Timestamp(row[\"withdrawal_date\"])\n",
    "        installation_date = pd.Timestamp(row[\"installation_date\"])\n",
    "        total_time_hour = (withdrawal_date - installation_date).total_seconds() / 3600 + 4\n",
    "\n",
    "        if total_time_hour >= window_time * 24 and percentageMissingValues < 40:\n",
    "            id_list.append(encounterId)\n",
    "            \n",
    "            delai_sortie_deces = df_death.loc[df_death[\"encounterId\"] == int(encounterId), \"delai_sortie_deces\"].to_numpy()[0]\n",
    "            if delai_sortie_deces <= 3:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "    \n",
    "    return target, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [00:14<00:00, 26.31it/s]\n",
      "100%|██████████| 287/287 [00:28<00:00, 10.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dataGroup = \"dataECMO\"\n",
    "# dataGroup = \"dataRangueil\"\n",
    "\n",
    "window_time_days = 5\n",
    "target, id_list = prepareDeathList(dataGroup, window_time_days)\n",
    "data = prepareData(dataGroup, id_list, window_time_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs, model_name, test_size, val_size, verbose):\n",
    "    # Split data into training and testing sets\n",
    "    x_train, x_test, x_val, y_train, y_test, y_val = split_train_test_val(data, target, test_size=test_size, val_size=val_size)\n",
    "\n",
    "    num_samples = len(target)\n",
    "    num_timesteps = 24 * window_time_days\n",
    "    num_features = np.size(x_train,2)\n",
    "    num_static_features = 3\n",
    "    \n",
    "    batch_size = 32\n",
    "\n",
    "    proportion_1 = np.sum(y_train)/np.size(y_train)\n",
    "    proportion_0 = 1 - proportion_1\n",
    "\n",
    "    class_weights = torch.tensor([1/proportion_0, 1/proportion_1], dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the CNN model\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 = nn.Conv1d(in_channels=num_features-num_static_features, out_channels=8, kernel_size=1)\n",
    "            self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "            self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=1)\n",
    "            self.fc1 = nn.Linear(480 , 16)  # Adjust input size based on your data dimensions\n",
    "            self.fc2 = nn.Linear(16+num_static_features, 1)  # For binary classification\n",
    "\n",
    "        def forward(self, x):\n",
    "            lstm_input = x[:, :-num_static_features, :]\n",
    "            static_input = x[:, -num_static_features:, 0]\n",
    "\n",
    "            out = self.pool(nn.functional.relu(self.conv1(lstm_input)))\n",
    "            out = self.pool(nn.functional.relu(self.conv2(out)))\n",
    "            out = torch.flatten(out, 1)\n",
    "            out = nn.functional.relu(self.fc1(out))\n",
    "            out = torch.cat((out, static_input), dim=1)\n",
    "            out = self.fc2(out)\n",
    "            # out = torch.sigmoid(self.fc2(out))  \n",
    "            return out\n",
    "\n",
    "    class CNN2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN2, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(1, 3), padding=1)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "            self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(1, 3), padding=1)\n",
    "            self.fc1 = nn.Linear(5760, 4)  # Adjust input size based on your data dimensions\n",
    "            self.fc2 = nn.Linear(4, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(1)  # Add a channel dimension\n",
    "            x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "            x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "            # x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = nn.functional.relu(self.fc1(x))\n",
    "            # x = nn.functional.sigmoid(self.fc2(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size, num_static_features):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            # self.fc1 = nn.Linear(hidden_size + num_static_features, 20)\n",
    "            # self.fc2 = nn.Linear(20, output_size)\n",
    "            self.fc2 = nn.Linear(hidden_size + num_static_features, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            lstm_input = x[:, :, :-num_static_features]\n",
    "            static_input = x[:, 0, -num_static_features:]\n",
    "\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            \n",
    "            out, _ = self.lstm(lstm_input, (h0, c0))\n",
    "            out = out[:, -1, :]  # Take the output of the last time step\n",
    "\n",
    "            out = torch.cat((out, static_input), dim=1)\n",
    "            # out = torch.relu(self.fc1(out))\n",
    "\n",
    "            # out = nn.functional.sigmoid(self.fc2(out))\n",
    "            out = self.fc2(out)\n",
    "            return out\n",
    "    \n",
    "    # Instantiate the model\n",
    "    if model_name == \"CNN\":\n",
    "        model = CNN()\n",
    "\n",
    "        # if verbose:\n",
    "        #     print(torchinfo.summary(model, input_size=(batch_size, num_features, num_timesteps)))\n",
    "    \n",
    "    if model_name == \"CNN2\":\n",
    "            model = CNN2()\n",
    "\n",
    "            if verbose:\n",
    "                print(torchinfo.summary(model, input_size=(batch_size, num_timesteps, num_features)))\n",
    "\n",
    "    elif model_name == \"LSTM\":\n",
    "        input_size = num_features-num_static_features\n",
    "        hidden_size = 32\n",
    "        num_layers = 2\n",
    "        output_size = 1\n",
    "\n",
    "        model = LSTMModel(input_size, hidden_size, num_layers, output_size, num_static_features)\n",
    "\n",
    "        if verbose:\n",
    "            print(torchinfo.summary(model, input_size=(batch_size, num_timesteps, num_features)))\n",
    "\n",
    "\n",
    "    class FocalLoss(nn.Module):\n",
    "        def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "            super(FocalLoss, self).__init__()\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "            self.reduction = reduction\n",
    "\n",
    "        def forward(self, inputs, targets):\n",
    "            # Apply sigmoid to inputs if not using BCEWithLogitsLoss\n",
    "            inputs = torch.sigmoid(inputs)\n",
    "            \n",
    "            # Flatten the inputs and targets\n",
    "            inputs = inputs.view(-1)\n",
    "            targets = targets.view(-1)\n",
    "            \n",
    "            # Compute the binary cross entropy loss\n",
    "            BCE_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "            \n",
    "            # Compute the focal loss component\n",
    "            pt = torch.where(targets == 1, inputs, 1 - inputs)\n",
    "            F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "            \n",
    "            if self.reduction == 'mean':\n",
    "                return F_loss.mean()\n",
    "            elif self.reduction == 'sum':\n",
    "                return F_loss.sum()\n",
    "            else:\n",
    "                return F_loss\n",
    "    # Define loss function and optimizer\n",
    "    # criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "    # Define weighted binary cross-entropy loss function\n",
    "    # criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "    criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # inputs = inputs.permute(0, 2, 1)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            predicted = torch.round(nn.functional.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train loss: {running_loss}, Train Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "        # Validation\n",
    "        if np.size(y_val) > 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    # inputs = inputs.permute(0, 2, 1)\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    val_loss += criterion(outputs, labels).item()\n",
    "                    predicted = torch.round(nn.functional.sigmoid(outputs))\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            if verbose:\n",
    "                print(f\"Validation Loss: {val_loss}, Accuracy on validation set: {100 * correct / total}%\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # inputs = inputs.permute(0, 2, 1)\n",
    "            outputs = model(inputs)\n",
    "            true_labels.extend(labels.numpy())\n",
    "            predictions.extend(outputs.numpy())\n",
    "\n",
    "    # Calculate AUROC score\n",
    "    auroc = roc_auc_score(true_labels, predictions)\n",
    "    # if verbose:\n",
    "    # print(\"Test AUROC score:\", auroc)\n",
    "    return auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LSTMModel                                [32, 1]                   --\n",
      "├─LSTM: 1-1                              [32, 120, 32]             13,568\n",
      "├─Linear: 1-2                            [32, 1]                   36\n",
      "==========================================================================================\n",
      "Total params: 13,604\n",
      "Trainable params: 13,604\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 52.10\n",
      "==========================================================================================\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 0.98\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 1.18\n",
      "==========================================================================================\n",
      "Epoch 1/100, Train loss: 1.4053281843662262, Train Accuracy: 52.38095238095238%\n",
      "Validation Loss: 0.3646904081106186, Accuracy on validation set: 46.42857142857143%\n",
      "Epoch 2/100, Train loss: 1.3033978641033173, Train Accuracy: 65.36796536796537%\n",
      "Validation Loss: 0.383168026804924, Accuracy on validation set: 50.0%\n",
      "Epoch 3/100, Train loss: 1.201858714222908, Train Accuracy: 74.45887445887446%\n",
      "Validation Loss: 0.42604784667491913, Accuracy on validation set: 50.0%\n",
      "Epoch 4/100, Train loss: 1.1544236242771149, Train Accuracy: 77.05627705627705%\n",
      "Validation Loss: 0.48837612569332123, Accuracy on validation set: 50.0%\n",
      "Epoch 5/100, Train loss: 1.071232222020626, Train Accuracy: 77.05627705627705%\n",
      "Validation Loss: 0.5006518065929413, Accuracy on validation set: 50.0%\n",
      "Epoch 6/100, Train loss: 0.9993515275418758, Train Accuracy: 76.62337662337663%\n",
      "Validation Loss: 0.49189379811286926, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 7/100, Train loss: 1.09132781624794, Train Accuracy: 75.75757575757575%\n",
      "Validation Loss: 0.5195250362157822, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 8/100, Train loss: 0.9841875955462456, Train Accuracy: 77.05627705627705%\n",
      "Validation Loss: 0.4894946664571762, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 9/100, Train loss: 0.9632545784115791, Train Accuracy: 77.4891774891775%\n",
      "Validation Loss: 0.5099713504314423, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 10/100, Train loss: 1.030436597764492, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.5211845934391022, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 11/100, Train loss: 0.9419450610876083, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.5004042088985443, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 12/100, Train loss: 0.9244680255651474, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.5099330246448517, Accuracy on validation set: 50.0%\n",
      "Epoch 13/100, Train loss: 0.9263829737901688, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.507050096988678, Accuracy on validation set: 50.0%\n",
      "Epoch 14/100, Train loss: 0.9383528307080269, Train Accuracy: 78.78787878787878%\n",
      "Validation Loss: 0.5077223926782608, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 15/100, Train loss: 0.962662048637867, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.49412599205970764, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 16/100, Train loss: 0.8845316544175148, Train Accuracy: 77.4891774891775%\n",
      "Validation Loss: 0.5099978297948837, Accuracy on validation set: 50.0%\n",
      "Epoch 17/100, Train loss: 0.8834236338734627, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.5190852135419846, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 18/100, Train loss: 0.9265309795737267, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.559278279542923, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 19/100, Train loss: 0.8639580011367798, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.5547462403774261, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 20/100, Train loss: 0.9243528842926025, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.5897476226091385, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 21/100, Train loss: 0.875092476606369, Train Accuracy: 79.22077922077922%\n",
      "Validation Loss: 0.5391278862953186, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 22/100, Train loss: 0.8863143846392632, Train Accuracy: 77.92207792207792%\n",
      "Validation Loss: 0.6440107375383377, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 23/100, Train loss: 0.8694851100444794, Train Accuracy: 78.78787878787878%\n",
      "Validation Loss: 0.5797214806079865, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 24/100, Train loss: 0.8449467644095421, Train Accuracy: 78.78787878787878%\n",
      "Validation Loss: 0.6971933096647263, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 25/100, Train loss: 0.8013859279453754, Train Accuracy: 78.35497835497836%\n",
      "Validation Loss: 0.6794547438621521, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 26/100, Train loss: 0.9070254564285278, Train Accuracy: 78.78787878787878%\n",
      "Validation Loss: 0.6746525466442108, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 27/100, Train loss: 0.8876383081078529, Train Accuracy: 78.78787878787878%\n",
      "Validation Loss: 0.46530652046203613, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 28/100, Train loss: 0.8925051763653755, Train Accuracy: 80.51948051948052%\n",
      "Validation Loss: 0.5630483776330948, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 29/100, Train loss: 0.7997612245380878, Train Accuracy: 80.08658008658008%\n",
      "Validation Loss: 0.627563402056694, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 30/100, Train loss: 0.8208035528659821, Train Accuracy: 79.22077922077922%\n",
      "Validation Loss: 0.6580831557512283, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 31/100, Train loss: 0.766619686037302, Train Accuracy: 79.22077922077922%\n",
      "Validation Loss: 0.6639560163021088, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 32/100, Train loss: 0.8177800923585892, Train Accuracy: 78.78787878787878%\n",
      "Validation Loss: 0.676083505153656, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 33/100, Train loss: 0.7377513758838177, Train Accuracy: 78.78787878787878%\n",
      "Validation Loss: 0.7114930301904678, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 34/100, Train loss: 0.7535033077001572, Train Accuracy: 80.08658008658008%\n",
      "Validation Loss: 0.7895602285861969, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 35/100, Train loss: 0.7754817977547646, Train Accuracy: 80.08658008658008%\n",
      "Validation Loss: 0.8206191956996918, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 36/100, Train loss: 0.8241006284952164, Train Accuracy: 80.51948051948052%\n",
      "Validation Loss: 0.765378326177597, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 37/100, Train loss: 0.8056686669588089, Train Accuracy: 79.65367965367966%\n",
      "Validation Loss: 0.753452867269516, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 38/100, Train loss: 0.7900992035865784, Train Accuracy: 79.22077922077922%\n",
      "Validation Loss: 0.7885903418064117, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 39/100, Train loss: 0.7695222944021225, Train Accuracy: 80.08658008658008%\n",
      "Validation Loss: 0.8675343990325928, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 40/100, Train loss: 0.737921267747879, Train Accuracy: 80.95238095238095%\n",
      "Validation Loss: 0.8840300142765045, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 41/100, Train loss: 0.7189104408025742, Train Accuracy: 80.95238095238095%\n",
      "Validation Loss: 0.8704820275306702, Accuracy on validation set: 55.357142857142854%\n",
      "Epoch 42/100, Train loss: 0.7387371025979519, Train Accuracy: 80.51948051948052%\n",
      "Validation Loss: 0.8556820452213287, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 43/100, Train loss: 0.7092059701681137, Train Accuracy: 81.38528138528139%\n",
      "Validation Loss: 0.8617869019508362, Accuracy on validation set: 55.357142857142854%\n",
      "Epoch 44/100, Train loss: 0.6710974611341953, Train Accuracy: 82.25108225108225%\n",
      "Validation Loss: 0.9795984923839569, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 45/100, Train loss: 0.7670306041836739, Train Accuracy: 81.81818181818181%\n",
      "Validation Loss: 0.9149337112903595, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 46/100, Train loss: 0.7454214841127396, Train Accuracy: 81.81818181818181%\n",
      "Validation Loss: 0.899817168712616, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 47/100, Train loss: 0.6605323702096939, Train Accuracy: 82.25108225108225%\n",
      "Validation Loss: 0.9521075487136841, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 48/100, Train loss: 0.6529519259929657, Train Accuracy: 82.25108225108225%\n",
      "Validation Loss: 0.8242382705211639, Accuracy on validation set: 55.357142857142854%\n",
      "Epoch 49/100, Train loss: 0.6143287308514118, Train Accuracy: 83.11688311688312%\n",
      "Validation Loss: 0.8584309220314026, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 50/100, Train loss: 0.6709495037794113, Train Accuracy: 83.54978354978356%\n",
      "Validation Loss: 0.9231866002082825, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 51/100, Train loss: 0.6035271175205708, Train Accuracy: 83.54978354978356%\n",
      "Validation Loss: 0.6335777342319489, Accuracy on validation set: 55.357142857142854%\n",
      "Epoch 52/100, Train loss: 0.6059647165238857, Train Accuracy: 83.54978354978356%\n",
      "Validation Loss: 0.8021077215671539, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 53/100, Train loss: 0.5941690690815449, Train Accuracy: 83.11688311688312%\n",
      "Validation Loss: 0.924901008605957, Accuracy on validation set: 50.0%\n",
      "Epoch 54/100, Train loss: 0.5883277282118797, Train Accuracy: 83.54978354978356%\n",
      "Validation Loss: 1.0180242955684662, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 55/100, Train loss: 0.5091274250298738, Train Accuracy: 85.28138528138528%\n",
      "Validation Loss: 1.1434995234012604, Accuracy on validation set: 50.0%\n",
      "Epoch 56/100, Train loss: 0.5985868200659752, Train Accuracy: 86.58008658008659%\n",
      "Validation Loss: 1.3321903049945831, Accuracy on validation set: 50.0%\n",
      "Epoch 57/100, Train loss: 0.49789347499608994, Train Accuracy: 87.01298701298701%\n",
      "Validation Loss: 0.8680508732795715, Accuracy on validation set: 55.357142857142854%\n",
      "Epoch 58/100, Train loss: 0.5569037683308125, Train Accuracy: 87.87878787878788%\n",
      "Validation Loss: 1.2449877560138702, Accuracy on validation set: 55.357142857142854%\n",
      "Epoch 59/100, Train loss: 0.4530635476112366, Train Accuracy: 87.01298701298701%\n",
      "Validation Loss: 1.2093080580234528, Accuracy on validation set: 50.0%\n",
      "Epoch 60/100, Train loss: 0.4653882198035717, Train Accuracy: 87.44588744588745%\n",
      "Validation Loss: 1.123337298631668, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 61/100, Train loss: 0.4911551773548126, Train Accuracy: 87.87878787878788%\n",
      "Validation Loss: 1.4564902782440186, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 62/100, Train loss: 0.9435599483549595, Train Accuracy: 88.74458874458874%\n",
      "Validation Loss: 1.506589114665985, Accuracy on validation set: 50.0%\n",
      "Epoch 63/100, Train loss: 0.472405094653368, Train Accuracy: 88.74458874458874%\n",
      "Validation Loss: 1.2592978179454803, Accuracy on validation set: 50.0%\n",
      "Epoch 64/100, Train loss: 0.6852873302996159, Train Accuracy: 89.6103896103896%\n",
      "Validation Loss: 1.0040428340435028, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 65/100, Train loss: 0.4772928059101105, Train Accuracy: 88.31168831168831%\n",
      "Validation Loss: 0.7991829812526703, Accuracy on validation set: 46.42857142857143%\n",
      "Epoch 66/100, Train loss: 0.4865396246314049, Train Accuracy: 87.01298701298701%\n",
      "Validation Loss: 0.8438178896903992, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 67/100, Train loss: 0.44905757904052734, Train Accuracy: 88.31168831168831%\n",
      "Validation Loss: 0.8008752763271332, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 68/100, Train loss: 0.45581841468811035, Train Accuracy: 89.6103896103896%\n",
      "Validation Loss: 1.009773850440979, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 69/100, Train loss: 0.4345645532011986, Train Accuracy: 90.9090909090909%\n",
      "Validation Loss: 1.013112097978592, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 70/100, Train loss: 0.42718715965747833, Train Accuracy: 91.34199134199135%\n",
      "Validation Loss: 1.1353445947170258, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 71/100, Train loss: 0.36727840453386307, Train Accuracy: 91.34199134199135%\n",
      "Validation Loss: 1.1535664796829224, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 72/100, Train loss: 0.3645315859466791, Train Accuracy: 90.9090909090909%\n",
      "Validation Loss: 1.254473477602005, Accuracy on validation set: 50.0%\n",
      "Epoch 73/100, Train loss: 0.33322674594819546, Train Accuracy: 92.64069264069263%\n",
      "Validation Loss: 1.2205955386161804, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 74/100, Train loss: 0.2972138598561287, Train Accuracy: 92.64069264069263%\n",
      "Validation Loss: 1.2250332832336426, Accuracy on validation set: 53.57142857142857%\n",
      "Epoch 75/100, Train loss: 0.3022648375481367, Train Accuracy: 93.07359307359307%\n",
      "Validation Loss: 1.4817805886268616, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 76/100, Train loss: 0.3117350023239851, Train Accuracy: 93.07359307359307%\n",
      "Validation Loss: 1.4498323798179626, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 77/100, Train loss: 0.3294508960098028, Train Accuracy: 93.07359307359307%\n",
      "Validation Loss: 1.5695606470108032, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 78/100, Train loss: 0.4087937343865633, Train Accuracy: 90.9090909090909%\n",
      "Validation Loss: 1.1942689716815948, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 79/100, Train loss: 0.4421355612576008, Train Accuracy: 89.6103896103896%\n",
      "Validation Loss: 1.2459109127521515, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 80/100, Train loss: 0.5275375582277775, Train Accuracy: 86.14718614718615%\n",
      "Validation Loss: 1.4217203259468079, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 81/100, Train loss: 0.39251336827874184, Train Accuracy: 89.17748917748918%\n",
      "Validation Loss: 1.4419110119342804, Accuracy on validation set: 44.642857142857146%\n",
      "Epoch 82/100, Train loss: 0.3636744264513254, Train Accuracy: 90.9090909090909%\n",
      "Validation Loss: 1.4728375971317291, Accuracy on validation set: 50.0%\n",
      "Epoch 83/100, Train loss: 0.3469605930149555, Train Accuracy: 91.34199134199135%\n",
      "Validation Loss: 1.6295198202133179, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 84/100, Train loss: 0.3505035061389208, Train Accuracy: 92.64069264069263%\n",
      "Validation Loss: 1.5041896402835846, Accuracy on validation set: 46.42857142857143%\n",
      "Epoch 85/100, Train loss: 0.29700513929128647, Train Accuracy: 93.93939393939394%\n",
      "Validation Loss: 1.4423438906669617, Accuracy on validation set: 50.0%\n",
      "Epoch 86/100, Train loss: 0.28916942328214645, Train Accuracy: 93.50649350649351%\n",
      "Validation Loss: 1.5742130875587463, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 87/100, Train loss: 0.26315613463521004, Train Accuracy: 94.8051948051948%\n",
      "Validation Loss: 1.5297194123268127, Accuracy on validation set: 50.0%\n",
      "Epoch 88/100, Train loss: 0.2705544028431177, Train Accuracy: 94.8051948051948%\n",
      "Validation Loss: 1.542655110359192, Accuracy on validation set: 50.0%\n",
      "Epoch 89/100, Train loss: 0.29800969921052456, Train Accuracy: 95.67099567099567%\n",
      "Validation Loss: 1.670874536037445, Accuracy on validation set: 50.0%\n",
      "Epoch 90/100, Train loss: 0.3877526316791773, Train Accuracy: 94.37229437229438%\n",
      "Validation Loss: 1.2569509744644165, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 91/100, Train loss: 0.49191707372665405, Train Accuracy: 89.17748917748918%\n",
      "Validation Loss: 1.3735352754592896, Accuracy on validation set: 46.42857142857143%\n",
      "Epoch 92/100, Train loss: 0.5455876365303993, Train Accuracy: 86.58008658008659%\n",
      "Validation Loss: 1.3886430263519287, Accuracy on validation set: 44.642857142857146%\n",
      "Epoch 93/100, Train loss: 0.6142819747328758, Train Accuracy: 85.71428571428571%\n",
      "Validation Loss: 1.2810729146003723, Accuracy on validation set: 46.42857142857143%\n",
      "Epoch 94/100, Train loss: 0.5485839694738388, Train Accuracy: 88.74458874458874%\n",
      "Validation Loss: 1.079238474369049, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 95/100, Train loss: 0.4865214675664902, Train Accuracy: 89.17748917748918%\n",
      "Validation Loss: 1.1617383062839508, Accuracy on validation set: 51.785714285714285%\n",
      "Epoch 96/100, Train loss: 0.3969414606690407, Train Accuracy: 91.34199134199135%\n",
      "Validation Loss: 1.3282285332679749, Accuracy on validation set: 48.214285714285715%\n",
      "Epoch 97/100, Train loss: 0.3781806714832783, Train Accuracy: 89.6103896103896%\n",
      "Validation Loss: 1.4202911853790283, Accuracy on validation set: 46.42857142857143%\n",
      "Epoch 98/100, Train loss: 0.38285598531365395, Train Accuracy: 91.34199134199135%\n",
      "Validation Loss: 1.3389847874641418, Accuracy on validation set: 42.857142857142854%\n",
      "Epoch 99/100, Train loss: 0.37324431352317333, Train Accuracy: 90.47619047619048%\n",
      "Validation Loss: 1.300028532743454, Accuracy on validation set: 44.642857142857146%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:25<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Train loss: 0.35669493302702904, Train Accuracy: 91.77489177489177%\n",
      "Validation Loss: 1.5006038546562195, Accuracy on validation set: 41.07142857142857%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m aurocs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_train), total\u001b[38;5;241m=\u001b[39mnum_train):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# auroc = train_model(num_epochs=15, model_name=\"LSTM\", test_size=0.2, val_size=0.0, verbose=False)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     auroc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     aurocs\u001b[38;5;241m.\u001b[39mappend(auroc)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUROC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(aurocs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 219\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(num_epochs, model_name, test_size, val_size, verbose)\u001b[0m\n\u001b[0;32m    216\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mextend(outputs\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Calculate AUROC score\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m auroc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# if verbose:\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# print(\"Test AUROC score:\", auroc)\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m auroc\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:618\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \\\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03mfrom prediction scores.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;124;03marray([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    617\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 618\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m y_score \u001b[38;5;241m=\u001b[39m check_array(y_score, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    622\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    623\u001b[0m ):\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\01821191\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "num_train = 100\n",
    "\n",
    "aurocs = []\n",
    "for i in tqdm(range(num_train), total=num_train):\n",
    "    # auroc = train_model(num_epochs=15, model_name=\"LSTM\", test_size=0.2, val_size=0.0, verbose=False)\n",
    "    auroc = train_model(num_epochs=100, model_name=\"LSTM\", test_size=0.0, val_size=0.2, verbose=True)\n",
    "    aurocs.append(auroc)\n",
    "    print(f\"AUROC: {np.mean(aurocs)}\")\n",
    "\n",
    "print(f\"AUROC: {np.mean(aurocs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[244], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241m.\u001b[39msize([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
