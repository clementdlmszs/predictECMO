{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, target, test_size):\n",
    "    nb_samples = len(target)\n",
    "    nb_test = int(test_size * nb_samples)\n",
    "\n",
    "    shuffle = list(range(nb_samples))\n",
    "    random.shuffle(shuffle)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    \n",
    "    nb_0 = 0\n",
    "    nb_1 = 0\n",
    "\n",
    "    for idx in shuffle:\n",
    "        if nb_0 < (nb_test//2) and target[idx]==0:\n",
    "            y_test.append(0)\n",
    "            X_test.append(data[idx])\n",
    "            nb_0 += 1\n",
    "        elif nb_1 < (nb_test//2) and target[idx]==1:\n",
    "            y_test.append(1)\n",
    "            X_test.append(data[idx])\n",
    "            nb_1 += 1\n",
    "        else:\n",
    "            y_train.append(target[idx])\n",
    "            X_train.append(data[idx])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def tnr_score(y_test, y_pred):\n",
    "    y_t = np.array(y_test)\n",
    "    y_p = np.array(y_pred)\n",
    "    tn = np.sum((1-y_t)*(1-y_p))\n",
    "    fp = np.sum(y_p*(1-y_t))\n",
    "    if (tn + fp) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateData(dataGroup, id_list, window_time):\n",
    "    \n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../dataECMO/\"\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "\n",
    "    patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "\n",
    "    finalDataPath = dataPath + \"finalData/\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for encounterId in tqdm(id_list, total=len(id_list)):\n",
    "\n",
    "        df_mask = pd.read_parquet(finalDataPath + encounterId + \"/mask.parquet\")\n",
    "        df_dynamic = pd.read_parquet(finalDataPath + encounterId + \"/dynamic.parquet\")\n",
    "        df_static = pd.read_parquet(finalDataPath + encounterId + \"/static.parquet\")\n",
    "        \n",
    "        data_patient = []\n",
    "\n",
    "        df_dynamic_masked = df_dynamic.iloc[:(window_time*24)].mask(df_mask.iloc[:(window_time*24)] == 0)\n",
    "\n",
    "        idx_variables_kept = [0,1,2,3,4,5,6,7,8]\n",
    "        # idx_variables_kept = [0,1,3,4,6,7,8]\n",
    "        df_dynamic_masked = df_dynamic_masked.iloc[:,idx_variables_kept]\n",
    "        df_dynamic = df_dynamic.iloc[:,idx_variables_kept]\n",
    "\n",
    "        statics = list(df_static.to_numpy()[0])\n",
    "        # CAS OU TOUTE UNE COLONNE EST MASQUEE ?\n",
    "        mean = df_dynamic_masked.mean().tolist()\n",
    "        sd = df_dynamic_masked.std().tolist()\n",
    "        median = df_dynamic_masked.median().tolist()\n",
    "        maxi = df_dynamic_masked.max().tolist()\n",
    "        mini = df_dynamic_masked.min().tolist()\n",
    "        first = list(df_dynamic.to_numpy()[0,:])\n",
    "        last = list(df_dynamic.to_numpy()[window_time*24-1,:])\n",
    "\n",
    "        data_patient.extend(mean)\n",
    "        data_patient.extend(sd)\n",
    "        # data_patient.extend(median)\n",
    "        data_patient.extend(maxi)\n",
    "        data_patient.extend(mini)\n",
    "        # data_patient.extend(first)\n",
    "        # data_patient.extend(last)\n",
    "        data_patient.extend(statics)\n",
    "\n",
    "        data.append(data_patient)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prepareDeathList(dataGroup, window_time):\n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../dataECMO/\"\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "    \n",
    "    patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "\n",
    "    df_death = pd.read_csv(dataPath + \"delais_deces.csv\")\n",
    "    \n",
    "    nb_patients = len(patients_df)\n",
    "\n",
    "    target = []\n",
    "    id_list = []\n",
    "\n",
    "    for _, row in tqdm(patients_df.iterrows(), total=nb_patients):\n",
    "        encounterId = str(row[\"encounterId\"])\n",
    "        \n",
    "        df_mask = pd.read_parquet(dataPath + \"finalData/\" + encounterId + \"/mask.parquet\")\n",
    "        total_true_values = df_mask.values.sum()\n",
    "        total_values = df_mask.values.size\n",
    "        percentageMissingValues = (total_values-total_true_values)/total_values * 100\n",
    "        \n",
    "        withdrawal_date = pd.Timestamp(row[\"withdrawal_date\"])\n",
    "        installation_date = pd.Timestamp(row[\"installation_date\"])\n",
    "        total_time_hour = (withdrawal_date - installation_date).total_seconds() / 3600 + 4\n",
    "\n",
    "        if total_time_hour >= window_time * 24 and percentageMissingValues < 40:\n",
    "            id_list.append(encounterId)\n",
    "            \n",
    "            delai_sortie_deces = df_death.loc[df_death[\"encounterId\"] == int(encounterId), \"delai_sortie_deces\"].to_numpy()[0]\n",
    "            if delai_sortie_deces <= 1:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "    \n",
    "    return target, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2150/2150 [01:07<00:00, 31.93it/s]\n",
      "100%|██████████| 1794/1794 [02:48<00:00, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb patients: 1794\n",
      "Nb Deceased: 478\n",
      "Nb Survived: 1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataGroup = \"dataECMO\"\n",
    "dataGroup = \"dataRangueil\"\n",
    "\n",
    "window_time_days = 5\n",
    "target, id_list = prepareDeathList(dataGroup, window_time_days)\n",
    "data = aggregateData(dataGroup, id_list, window_time_days)\n",
    "\n",
    "print(f\"Nb patients: {len(target)}\")\n",
    "print(f\"Nb Deceased: {np.sum(target)}\")\n",
    "print(f\"Nb Survived: {len(target) - np.sum(target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.7217\n",
      "Precision: 0.4394\n",
      "Recall: 0.6104\n",
      "Specificity: 0.7168\n",
      "Accuracy: 0.6883\n",
      "F1 Score: 0.5097\n"
     ]
    }
   ],
   "source": [
    "aurocs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "tnrs = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "    while np.sum(y_test) == 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "    # y_train_np = np.array(y_train)\n",
    "    # nb_tot = np.size(y_train_np)\n",
    "    # nb_1 = np.sum(y_train_np)\n",
    "    # nb_0 = nb_tot - nb_1\n",
    "\n",
    "    # idx = 0\n",
    "    # while nb_1 < nb_0:\n",
    "    #     if idx >= nb_tot:\n",
    "    #         idx = 0\n",
    "    #     elif y_train_np[idx] == 1:\n",
    "    #         X_train.append(data[idx])\n",
    "    #         y_train.append(1)\n",
    "    #         nb_1 += 1\n",
    "    #     idx += 1\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 5,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.75,\n",
    "        'verbose': -1,\n",
    "        'class_weight' : class_weights\n",
    "    }\n",
    "\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    tnr = tnr_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    aurocs.append(auroc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    tnrs.append(tnr)\n",
    "    f1s.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "print(f\"AUROC: {np.mean(aurocs):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f}\")\n",
    "print(f\"Specificity: {np.mean(tnrs):.4f}\")\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create a TreeExplainer object for the LightGBM model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(\u001b[43mclf\u001b[49m)\n\u001b[0;32m      6\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_test)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate SHAP values for the test set\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Create a TreeExplainer object for the LightGBM model\n",
    "explainer = shap.Explainer(clf)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer(np.array(X_test))\n",
    "\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.7195\n",
      "Precision: 0.5435\n",
      "Recall: 0.2805\n",
      "Specificity: 0.9139\n",
      "Accuracy: 0.7451\n",
      "F1 Score: 0.3678\n"
     ]
    }
   ],
   "source": [
    "aurocs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "tnrs = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "    while np.sum(y_test) == 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "    pos_weight = np.sum(np.array(y_train) == 0) / np.sum(np.array(y_train) == 1)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 5,\n",
    "        'verbosity': 0,\n",
    "        'n_estimator': 100\n",
    "    }\n",
    "\n",
    "    # Initialize and train the classifier\n",
    "    clf = xgb.XGBClassifier(**params)\n",
    "    # clf = xgb.XGBClassifier(\n",
    "    # learning_rate=0.1,\n",
    "    # max_depth=5,\n",
    "    # n_estimators=100,\n",
    "    # objective='binary:logistic',\n",
    "    # class_weights\n",
    "    # )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    tnr = tnr_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    aurocs.append(auroc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    tnrs.append(tnr)\n",
    "    f1s.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "print(f\"AUROC: {np.mean(aurocs):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f}\")\n",
    "print(f\"Specificity: {np.mean(tnrs):.4f}\")\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.4917\n",
      "Precision: 0.3609\n",
      "Recall: 0.3502\n",
      "Specificity: 0.6401\n",
      "Accuracy: 0.5326\n",
      "F1 Score: 0.3523\n"
     ]
    }
   ],
   "source": [
    "aurocs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "tnrs = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "    while np.sum(y_test) == 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "    base_classifier = DecisionTreeClassifier()\n",
    "\n",
    "    clf = BaggingClassifier(estimator=base_classifier, n_estimators=100)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    tnr = tnr_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    aurocs.append(auroc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    tnrs.append(tnr)\n",
    "    f1s.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "print(f\"AUROC: {np.mean(aurocs):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f}\")\n",
    "print(f\"Specificity: {np.mean(tnrs):.4f}\")\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.4986\n",
      "Precision: 0.3564\n",
      "Recall: 0.3546\n",
      "Specificity: 0.6447\n",
      "Accuracy: 0.5410\n",
      "F1 Score: 0.3511\n"
     ]
    }
   ],
   "source": [
    "aurocs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "tnrs = []\n",
    "accuracies = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "    while np.sum(y_test) == 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "    clf = LogisticRegression(solver='sag', penalty='l2', verbose=0, max_iter=1000)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    tnr = tnr_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    aurocs.append(auroc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    tnrs.append(tnr)\n",
    "    f1s.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "print(f\"AUROC: {np.mean(aurocs):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f}\")\n",
    "print(f\"Specificity: {np.mean(tnrs):.4f}\")\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"F1 Score: {np.mean(f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
