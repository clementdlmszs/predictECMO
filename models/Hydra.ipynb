{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "from aeon.regression.convolution_based import MultiRocketHydraRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tnr_score(y_test, y_pred):\n",
    "    y_t = np.array(y_test)\n",
    "    y_p = np.array(y_pred)\n",
    "    tn = np.sum((1-y_t)*(1-y_p))\n",
    "    fp = np.sum(y_p*(1-y_t))\n",
    "    if (tn + fp) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(dataGroup, id_list, window_time):\n",
    "    if dataGroup == \"dataMimic\":\n",
    "        dataPath = \"../Mimic/dataMimic/\"\n",
    "    elif dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../dataECMO/\"\n",
    "    else:\n",
    "        dataPath = \"../dataRea/\"\n",
    "\n",
    "    finalDataPath = dataPath + \"finalData/\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for encounterId in tqdm(id_list, total=len(id_list)):\n",
    "        \n",
    "        df_mask = pd.read_parquet(finalDataPath + encounterId + \"/mask.parquet\")\n",
    "        df_dynamic = pd.read_parquet(finalDataPath + encounterId + \"/dynamic.parquet\")\n",
    "        df_static = pd.read_parquet(finalDataPath + encounterId + \"/static.parquet\")\n",
    "\n",
    "        idx_variables_kept = [0,1,2,3,4,5,6,7,9,10,11]\n",
    "    \n",
    "        # Ajout des variables dynamiques\n",
    "        data_patient = df_dynamic.iloc[:(window_time*24), idx_variables_kept].to_numpy()\n",
    "    \n",
    "        # Ajout des variables statiques\n",
    "        for value in df_static.to_numpy()[0]:\n",
    "            new_column = np.ones(shape=(window_time*24,1)) * value\n",
    "            data_patient = np.append(data_patient, new_column, axis=1)\n",
    "\n",
    "        \n",
    "        data.append(data_patient)\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def prepareDeathList(dataGroup, window_time):\n",
    "    target = []\n",
    "    id_list = []\n",
    "    \n",
    "    if dataGroup == \"dataMimic\":\n",
    "        dataPath = \"../Mimic/dataMimic/\"\n",
    "        \n",
    "        patients_df = pd.read_csv(dataPath + \"ventiles.csv\")\n",
    "        # df_ventilation_start = pd.read_csv(dataPath + \"ventiles1.csv\")\n",
    "\n",
    "        nb_patients = len(patients_df)\n",
    "    \n",
    "        for index, row in tqdm(patients_df.iterrows(), total=nb_patients):\n",
    "            stay_id = str(row[\"stay_id\"])\n",
    "\n",
    "            df_mask = pd.read_parquet(dataPath + \"finalData/\" + stay_id + \"/mask.parquet\")\n",
    "            total_true_values = df_mask.values.sum()\n",
    "            total_values = df_mask.values.size\n",
    "            percentageMissingValues = (total_values-total_true_values)/total_values * 100\n",
    "\n",
    "            if percentageMissingValues < 40:\n",
    "                id_list.append(stay_id)\n",
    "                \n",
    "                label_death = int(row[\"label\"])\n",
    "                if label_death:\n",
    "                    target.append(1)\n",
    "                else:\n",
    "                    target.append(0)\n",
    "            \n",
    "        \n",
    "        return target, id_list\n",
    "    \n",
    "    if dataGroup == \"dataECMO\":\n",
    "        dataPath = \"../dataECMO/\"\n",
    "    elif dataGroup == \"dataRangueil\":\n",
    "        dataPath = \"../dataRea/\"\n",
    "\n",
    "    \n",
    "    patients_df = pd.read_parquet(dataPath + \"patients.parquet\")\n",
    "\n",
    "    df_death = pd.read_csv(dataPath + \"delais_deces.csv\")\n",
    "    \n",
    "    nb_patients = len(patients_df)\n",
    "\n",
    "    for _, row in tqdm(patients_df.iterrows(), total=nb_patients):\n",
    "        encounterId = str(row[\"encounterId\"])\n",
    "        \n",
    "        df_mask = pd.read_parquet(dataPath + \"finalData/\" + encounterId + \"/mask.parquet\")\n",
    "        total_true_values = df_mask.values.sum()\n",
    "        total_values = df_mask.values.size\n",
    "        percentageMissingValues = (total_values-total_true_values)/total_values * 100\n",
    "        \n",
    "        withdrawal_date = pd.Timestamp(row[\"withdrawal_date\"])\n",
    "        installation_date = pd.Timestamp(row[\"installation_date\"])\n",
    "        total_time_hour = (withdrawal_date - installation_date).total_seconds() / 3600 + 4\n",
    "\n",
    "        if total_time_hour >= window_time * 24 and percentageMissingValues < 40:\n",
    "            id_list.append(encounterId)\n",
    "            \n",
    "            delai_sortie_deces = df_death.loc[df_death[\"encounterId\"] == int(encounterId), \"delai_sortie_deces\"].to_numpy()[0]\n",
    "            delai_installation_deces = df_death.loc[df_death[\"encounterId\"] == int(encounterId), \"delai_installation_deces\"].to_numpy()[0]\n",
    "            if delai_sortie_deces <= 1: #and delai_installation_deces <= 35:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "    \n",
    "    return target, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 189/189 [00:00<00:00, 360.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 282.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECMO dataset size: 153 , num_deceased: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_ECMO, id_list_ECMO = prepareDeathList(\"dataECMO\", window_time=5)\n",
    "data_ECMO = prepareData(\"dataECMO\", id_list_ECMO, window_time=5)\n",
    "\n",
    "print(f\"ECMO dataset size: {len(target_ECMO)} , num_deceased: {np.sum(target_ECMO)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4245/4245 [00:05<00:00, 789.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4140/4140 [00:14<00:00, 281.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mimic dataset size: 4140 , num_deceased: 1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_Mimic, id_list_Mimic = prepareDeathList(\"dataMimic\", window_time=5)\n",
    "data_Mimic = prepareData(\"dataMimic\", id_list_Mimic, window_time=5)\n",
    "\n",
    "print(f\"Mimic dataset size: {len(target_Mimic)} , num_deceased: {np.sum(target_Mimic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import augmentation as aug\n",
    "\n",
    "def augment(X_train, Y_train):\n",
    "    indices = np.where(np.array(Y_train) == 1)\n",
    "    X_deceased = X_train[indices]\n",
    "    \n",
    "    # x_aug_1 = aug.magnitude_warp(X_deceased, sigma=0.2, knot=4)\n",
    "    # x_aug_2 = aug.window_warp(X_deceased)\n",
    "    x_aug_3 = aug.spawner(X_deceased, np.ones(np.shape(X_deceased)[0]))\n",
    "    # x_aug_3 = aug.wdba(X_deceased, np.ones(np.shape(X_deceased)[0]))\n",
    "\n",
    "    x_train = np.concatenate((X_train, x_aug_3))\n",
    "\n",
    "    y_train = np.concatenate((Y_train, np.ones(np.shape(x_train)[0]-np.shape(X_train)[0])))\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMIC without imputation\n",
    "saveDataPath = \"../final_datasets/Mimic/without_imputation/\"\n",
    "\n",
    "test_data_Mimic = np.load(saveDataPath + \"test_data.npy\")\n",
    "test_target_Mimic = np.load(saveDataPath + \"test_target.npy\")\n",
    "train_data_Mimic = np.load(saveDataPath + \"train_data.npy\")\n",
    "train_target_Mimic = np.load(saveDataPath + \"train_target.npy\")\n",
    "\n",
    "\n",
    "# MIMIC with imputation\n",
    "saveDataPath = \"../final_datasets/Mimic/with_imputation/\"\n",
    "\n",
    "test_data_Mimic_Imputed = np.load(saveDataPath + \"test_data.npy\")\n",
    "test_target_Mimic_Imputed = np.load(saveDataPath + \"test_target.npy\")\n",
    "train_data_Mimic_Imputed = np.load(saveDataPath + \"train_data.npy\")\n",
    "train_target_Mimic_Imputed = np.load(saveDataPath + \"train_target.npy\")\n",
    "\n",
    "\n",
    "# VENTILES without imputation\n",
    "saveDataPath = \"../final_datasets/Ventiles/without_imputation/\"\n",
    "\n",
    "test_data_Ventiles = np.load(saveDataPath + \"test_data.npy\")\n",
    "test_target_Ventiles = np.load(saveDataPath + \"test_target.npy\")\n",
    "train_data_Ventiles = np.load(saveDataPath + \"train_data.npy\")\n",
    "train_target_Ventiles = np.load(saveDataPath + \"train_target.npy\")\n",
    "\n",
    "\n",
    "# VENTILES with imputation\n",
    "saveDataPath = \"../final_datasets/Ventiles/with_imputation/\"\n",
    "\n",
    "test_data_Ventiles_Imputed = np.load(saveDataPath + \"test_data.npy\")\n",
    "test_target_Ventiles_Imputed = np.load(saveDataPath + \"test_target.npy\")\n",
    "train_data_Ventiles_Imputed = np.load(saveDataPath + \"train_data.npy\")\n",
    "train_target_Ventiles_Imputed = np.load(saveDataPath + \"train_target.npy\")\n",
    "\n",
    "\n",
    "# ECMO without imputation\n",
    "saveDataPath = \"../final_datasets/ECMO/\"\n",
    "\n",
    "data_ECMO = np.load(saveDataPath + \"data_ECMO.npy\")\n",
    "target_ECMO = np.load(saveDataPath + \"target_ECMO.npy\")\n",
    "\n",
    "\n",
    "# ECMO with imputation\n",
    "saveDataPath = \"../final_datasets/ECMO/\"\n",
    "\n",
    "data_ECMO_Imputed = np.load(saveDataPath + \"data_ECMO_Imputed.npy\")\n",
    "target_ECMO_Imputed = np.load(saveDataPath + \"target_ECMO_Imputed.npy\")\n",
    "\n",
    "\n",
    "# ECMO_M without imputation\n",
    "saveDataPath = \"../final_datasets/ECMO_M/\"\n",
    "\n",
    "data_ECMO_M = np.load(saveDataPath + \"data_ECMO_M.npy\")\n",
    "target_ECMO_M = np.load(saveDataPath + \"target_ECMO_M.npy\")\n",
    "\n",
    "\n",
    "# ECMO_M with imputation\n",
    "saveDataPath = \"../final_datasets/ECMO_M/\"\n",
    "\n",
    "data_ECMO_M_Imputed = np.load(saveDataPath + \"data_ECMO_M_Imputed.npy\")\n",
    "target_ECMO_M_Imputed = np.load(saveDataPath + \"target_ECMO_M_Imputed.npy\")\n",
    "\n",
    "\n",
    "# ECMO train/test without imputation\n",
    "saveDataPath = \"../final_datasets/ECMO/without_imputation2/\"\n",
    "\n",
    "test_data_ECMO = np.load(saveDataPath + \"test_data.npy\")\n",
    "test_target_ECMO = np.load(saveDataPath + \"test_target.npy\")\n",
    "train_data_ECMO = np.load(saveDataPath + \"train_data.npy\")\n",
    "train_target_ECMO = np.load(saveDataPath + \"train_target.npy\")\n",
    "\n",
    "\n",
    "# ECMO train/test with imputation\n",
    "saveDataPath = \"../final_datasets/ECMO/with_imputation2/\"\n",
    "\n",
    "test_data_ECMO_Imputed = np.load(saveDataPath + \"test_data.npy\")\n",
    "test_target_ECMO_Imputed = np.load(saveDataPath + \"test_target.npy\")\n",
    "train_data_ECMO_Imputed = np.load(saveDataPath + \"train_data.npy\")\n",
    "train_target_ECMO_Imputed = np.load(saveDataPath + \"train_target.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(4214, 120, 15)\n",
      "-14.549816\n"
     ]
    }
   ],
   "source": [
    "np.shape(train_data_Ventiles_Imputed)\n",
    "print(type(train_data_Ventiles_Imputed))\n",
    "print(np.shape(train_data_Ventiles_Imputed))\n",
    "print(np.min(train_data_Ventiles_Imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 120, 15)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ECMO.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6163971154519359\n",
      "0.6316366776354174\n",
      "0.641205162314173\n",
      "Total Mean AUROC: 0.630 ± 0.010\n"
     ]
    }
   ],
   "source": [
    "########### HYDRA-MR ############\n",
    "\n",
    "train_data = train_data_Mimic\n",
    "train_target = train_target_Mimic\n",
    "test_data = test_data_Mimic\n",
    "test_target = test_target_Mimic\n",
    "\n",
    "# train_data = train_data_Mimic_Imputed\n",
    "# train_target = train_target_Mimic_Imputed\n",
    "# test_data = test_data_Mimic_Imputed\n",
    "# test_target = test_target_Mimic_Imputed\n",
    "\n",
    "\n",
    "# train_data = train_data_Ventiles\n",
    "# train_target = train_target_Ventiles\n",
    "# test_data = test_data_Ventiles\n",
    "# test_target = test_target_Ventiles\n",
    "\n",
    "# train_data = train_data_Ventiles_Imputed\n",
    "# train_target = train_target_Ventiles_Imputed\n",
    "# test_data = test_data_Ventiles_Imputed\n",
    "# test_target = test_target_Ventiles_Imputed\n",
    "\n",
    "\n",
    "# train_data = np.concatenate((train_data_Mimic, test_data_Mimic))\n",
    "# train_target = np.concatenate((train_target_Mimic, test_target_Mimic))\n",
    "# test_data = data_ECMO\n",
    "# test_target = target_ECMO\n",
    "\n",
    "# train_data = np.concatenate((train_data_Ventiles, test_data_Ventiles))\n",
    "# train_target = np.concatenate((train_target_Ventiles, test_target_Ventiles))\n",
    "# test_data = data_ECMO\n",
    "# test_target = target_ECMO\n",
    "\n",
    "\n",
    "# train_data = np.concatenate((train_data_Mimic_Imputed, test_data_Mimic_Imputed))\n",
    "# train_target = np.concatenate((train_target_Mimic_Imputed, test_target_Mimic_Imputed))\n",
    "# test_data = data_ECMO_Imputed\n",
    "# test_target = target_ECMO_Imputed\n",
    "\n",
    "# train_data = np.concatenate((train_data_Ventiles_Imputed, test_data_Ventiles_Imputed))\n",
    "# train_target = np.concatenate((train_target_Ventiles_Imputed, test_target_Ventiles_Imputed))\n",
    "# test_data = data_ECMO_Imputed\n",
    "# test_target = target_ECMO_Imputed\n",
    "\n",
    "\n",
    "# train_data = train_data_ECMO\n",
    "# train_target = train_target_ECMO\n",
    "# test_data = test_data_ECMO\n",
    "# test_target = test_target_ECMO\n",
    "\n",
    "# train_data = train_data_ECMO_Imputed\n",
    "# train_target = train_target_ECMO_Imputed\n",
    "# test_data = test_data_ECMO_Imputed\n",
    "# test_target = test_target_ECMO_Imputed\n",
    "\n",
    "\n",
    "# train_data = data_ECMO\n",
    "# train_target = target_ECMO\n",
    "# test_data = []\n",
    "# test_target = []\n",
    "\n",
    "# train_data = data_ECMO_Imputed\n",
    "# train_target = target_ECMO_Imputed\n",
    "# test_data = []\n",
    "# test_target = []\n",
    "\n",
    "\n",
    "K = 3\n",
    "\n",
    "aurocs = []\n",
    "auroc_ecmo = 0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs = []\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "for j in range(K):\n",
    "    \n",
    "    clf = MultiRocketHydraRegressor(n_kernels=8, n_groups=64, n_jobs=1, random_state=None)\n",
    "\n",
    "    # print(np.shape(train_data))\n",
    "    # print(np.shape(train_target))\n",
    "    \n",
    "    # train_data, train_target = data_Mimic, np.array(target_Mimic)\n",
    "    clf.fit(train_data, train_target)\n",
    "    \n",
    "    y_pred_proba = clf.predict(test_data)\n",
    "    \n",
    "    auroc = roc_auc_score(test_target, y_pred_proba)\n",
    "    aurocs.append(auroc)\n",
    "    print(auroc)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(test_target, y_pred_proba)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0  # Ensure the curve starts at (0,0)\n",
    "\n",
    "    predictions_list = predictions_list + list(y_pred_proba)\n",
    "    labels_list = labels_list + list(test_target)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure the curve ends at (1,1)\n",
    "\n",
    "list_to_save = [aurocs, mean_tpr, predictions_list, labels_list]\n",
    "\n",
    "print(f\"Total Mean AUROC: {np.mean(aurocs):.3f} ± {np.std(aurocs):.3f}\")\n",
    "\n",
    "saved_results_folder = \"./saved_results/\"\n",
    "saved_file_name = \"Hydra_MR\"\n",
    "\n",
    "# with open(saved_results_folder + saved_file_name + \".pkl\", 'wb') as file:\n",
    "#     pickle.dump(list_to_save, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env0]",
   "language": "python",
   "name": "conda-env-env0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
